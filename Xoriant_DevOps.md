# Role: DevOps
# Experience: 3.9 YOE
# 1st Round Interview Questions:
---

‚Üí what are your job responsibilities? How are you managing things? How are you doing collaborative work? What kind of projects are you supporting? How do you handle day-to-day activities?

‚Üí For CI/CD are you using Jenkins? Let‚Äôs say I have Kubernetes environment might be I am going with DEV, UAT, and Production. How are you handling the pipelines? Are you having multiple Jenkins jobs triggered per environment, or is it a single job triggering from development to production? How is it happening?

‚Üí In your Kubernetes environment, how many services and pods are running? How many applications are actually deployed?

‚Üí How are you managing deployments? Do you have a single pipeline or multiple pipelines per branch?

‚Üí Let‚Äôs say I am moving services to production‚Äî6 out of 10 services‚Äîfrom dev to production. How are you managing this workflow?

‚Üí How do you come to know about the requirements? How do you move the code and understand how your Jenkins pipelines run? Once a Jenkins job is finished‚Äîlet‚Äôs say it builds and releases to Kubernetes‚Äîhow do you know things are working fine in production after deployment?

‚Üí For DEV, UAT, and Production, are you using separate Git branches? Why do you need separate branches?

‚Üí Let‚Äôs say you are moving code from development to pre-prod. Once pre-prod is fine, you move the code to the main/master branch for release. If something goes wrong in production and a hotfix/bugfix is needed‚Äîand rollback is not possible due to database or backend changes‚Äîwhat strategy do you follow to fix bugs in production?

‚Üí Let‚Äôs say you created a hotfix branch from master. After fixing the issue, do you merge it back to master and also into the development branch?

‚Üí How do you manage release versioning?

‚Üí Are you creating a single Docker image for all environments, or are you creating images separately for each environment?

‚Üí Can we use one image for the entire environment?

‚Üí When pushing images to Artifactory/Nexus, how are you managing the lifecycle of those images?

‚Üí Let‚Äôs say developers are creating multiple images. By the end of the month, there are many images in Nexus. How do you manage the image lifecycle? How do you decide which images need to be purged and which need to be kept? Does the same strategy apply for pre-prod and prod?

‚Üí How do you create a Dockerfile for Node.js? Which version do you use, what base image, and what is the final image size? How do you manage node_modules? Do you install node modules every time, or do you copy them from somewhere? How do you handle these things?

‚Üí If the image size becomes very large, how do you handle that?

‚Üí What are the things you do using Terraform?

‚Üí Can you give a recent example where you worked with Terraform, and what tasks you performed?

‚Üí Have you used Terraform workspaces?

‚Üí How do you differentiate environments within Terraform?

‚Üí How do you manage multiple environments using Terraform?

‚Üí Are you responsible for writing Terraform modules? For example, if you want to write a module for EC2, what best practices do you follow? How do you write reusable modules for different Terraform scripts, and what kind of standardization have you implemented?

‚Üí You also work with Python, right? How do you handle exceptions in Python? Can you give a simple example of an exception‚Äîwhere and why we use it, and where we should not use exceptions?

‚Üí How do you read a file in Python line by line?

‚Üí Have you worked with Lambdas?

‚Üí Do you know Lambda handlers? How do they work?

---

## Answers

---

## 1Ô∏è‚É£ Job Responsibilities, Collaboration & Day-to-Day Work

**Answer:**

My primary responsibility as a DevOps/DevSecOps engineer is to **build, maintain, and improve CI/CD pipelines**, manage **cloud and Kubernetes infrastructure**, and ensure **secure, reliable, and scalable deployments**.

On a day-to-day basis:

* I work closely with **developers** to understand application changes, build issues, and deployment needs.
* I collaborate with **QA/UAT teams** to support environment stability and release readiness.
* I coordinate with **security teams** for vulnerability scanning, compliance checks, and incident handling.
* I support **production systems**, monitor alerts, and handle incidents or performance issues.

Typical daily activities include:

* Monitoring Jenkins pipelines, Kubernetes clusters, and cloud resources
* Supporting deployments or hotfixes
* Debugging build or deployment failures
* Writing or improving automation using **Jenkins, Terraform, Ansible, or Python**
* Participating in **standups, release calls, and post-incident reviews**

The projects I support are mostly **microservices-based applications** running on Kubernetes with cloud infrastructure provisioned using Terraform.

---

## 2Ô∏è‚É£ CI/CD with Jenkins for DEV, UAT, PROD (Pipeline Design)

**Answer:**

Yes, we use **Jenkins for CI/CD** with Kubernetes environments like **DEV, UAT, and Production**.

We generally follow a **single Jenkins pipeline with environment-specific stages**, not completely separate jobs for each environment.

How it works:

* **CI stages** (build, unit tests, security scans, Docker image build) are triggered automatically on **code commit**.
* The pipeline builds **one immutable Docker image** and pushes it to Nexus/Artifactory.
* **CD stages** deploy the same image to:

  * DEV automatically
  * UAT after approval
  * Production after final approval

Environment-specific behavior is handled using:

* Different **values files (Helm)** or **Kubernetes manifests**
* Environment variables
* Separate namespaces or clusters

This approach ensures **consistency across environments** and avoids ‚Äúworks in dev but not in prod‚Äù issues.

---

## 3Ô∏è‚É£ Kubernetes Environment ‚Äì Services, Pods & Applications

**Answer:**

In a typical Kubernetes setup:

* We usually have **20‚Äì40 microservices** depending on the project
* Each service may run **2‚Äì5 pods per environment**
* So overall, we might see **100+ pods per environment**

Applications are usually:

* Independent microservices
* Exposed internally via ClusterIP
* Exposed externally via Ingress or LoadBalancer

We manage and monitor this using:

* `kubectl`
* Prometheus & Grafana
* Centralized logging (ELK or similar)

The exact numbers vary by environment:

* DEV has fewer replicas
* PROD has higher replicas for high availability

---

## 4Ô∏è‚É£ Deployment Strategy ‚Äì Single or Multiple Pipelines

**Answer:**

We mostly use a **single pipeline**, but it is **branch-aware and environment-aware**.

Pipeline behavior changes based on:

* Git branch
* Environment
* Approval gates

Example:

* Feature branches ‚Üí CI only
* Develop branch ‚Üí CI + deploy to DEV
* Release branch ‚Üí deploy to UAT
* Main/Master branch ‚Üí deploy to Production

This keeps the pipeline **centralized, manageable, and auditable**, instead of maintaining multiple Jenkins jobs.

---

## 5Ô∏è‚É£ Deploying Partial Services (6 out of 10 to Production)

**Answer:**

In microservices architecture, we **deploy services independently**.

If only **6 out of 10 services** need to go to production:

* Only those services are built and deployed
* Other services remain untouched

This is handled by:

* Separate repositories per service **OR**
* Mono-repo with service-level pipeline triggers

Each service has:

* Its own Docker image
* Its own Helm chart or Kubernetes manifest
* Its own deployment pipeline

So production deployment is **service-specific**, not environment-wide.

---

## 6Ô∏è‚É£ Requirements Gathering & Post-Deployment Validation

**Answer:**

Requirements come from:

* Jira tickets
* Release notes
* Sprint planning discussions
* Direct communication with developers or product owners

Once Jenkins finishes:

* We verify deployment using:

  * `kubectl rollout status`
  * Health check endpoints
  * Application logs
* We monitor:

  * Error rates
  * Pod restarts
  * Latency and resource usage

For production:

* We rely on **monitoring dashboards**
* Alerts from Prometheus or cloud monitoring
* Sometimes smoke tests or sanity checks

If everything is green, the deployment is considered successful.

---

## 7Ô∏è‚É£ Git Branching for DEV, UAT & PROD

**Answer:**

Yes, we use **separate Git branches**.

Typical setup:

* `develop` ‚Üí DEV
* `release` or `uat` ‚Üí UAT
* `main/master` ‚Üí Production

Why separate branches:

* Code stability increases as we move forward
* Production code remains protected
* Easier rollback and hotfix management
* Controlled release process

This avoids unstable code reaching production accidentally.

---

## 8Ô∏è‚É£ Production Bugfix When Rollback Is Not Possible

**Answer:**

If rollback is not possible due to:

* Database changes
* Backend API changes

We follow a **hotfix strategy**:

* Create a **hotfix branch from master**
* Apply minimal, targeted fix
* Test it quickly in a controlled environment
* Deploy directly to production

After production fix:

* The same fix is merged back into **develop**
* This ensures future releases include the fix

This is a very common real-world production scenario.

---

## 9Ô∏è‚É£ Hotfix Branch ‚Äì Merge Strategy

**Answer:**

Yes, after fixing the issue:

* The hotfix branch is merged into **master**
* And also merged into **develop**

Why:

* Master gets the immediate fix
* Develop stays in sync
* Avoids reintroducing the same bug in future releases

Skipping merge to develop can cause regression later.

---

## üîü Release Versioning Strategy

**Answer:**

We follow **semantic versioning**:

* `MAJOR.MINOR.PATCH`

Examples:

* `1.2.0` ‚Üí new feature release
* `1.2.1` ‚Üí bug fix
* `2.0.0` ‚Üí breaking change

Versioning is applied to:

* Git tags
* Docker images
* Helm chart versions

This helps with:

* Traceability
* Rollbacks
* Audit and compliance

---

## 1Ô∏è‚É£1Ô∏è‚É£ Single Docker Image vs Separate Images per Environment

**Answer:**

We follow the principle of **‚Äúbuild once, deploy everywhere.‚Äù**
So we **create a single Docker image** and deploy the same image to **DEV, UAT, and Production**.

Why:

* Ensures **consistency across environments**
* Avoids environment-specific bugs
* Improves traceability and rollback
* Aligns with DevOps best practices

The image remains the same; only **configuration changes per environment**, like:

* Environment variables
* ConfigMaps
* Secrets
* Helm values

---

## 1Ô∏è‚É£2Ô∏è‚É£ Can One Image Be Used Across All Environments?

**Answer:**

Yes, **one Docker image should be used for all environments**.

We never bake environment-specific values into the image.
Instead, we externalize configuration using:

* Kubernetes ConfigMaps
* Secrets
* Helm values files
* Runtime environment variables

This ensures:

* No rebuilds per environment
* Faster deployments
* Better security and auditability

---

## 1Ô∏è‚É£3Ô∏è‚É£ Image Lifecycle Management in Nexus / Artifactory

**Answer:**

We manage image lifecycle using:

* **Retention policies**
* **Tagging strategies**
* **Automated cleanup jobs**

Typical approach:

* Keep:

  * Latest N images (for example, last 20)
  * All production-tagged images
* Purge:

  * Old snapshots
  * Untagged images
  * Failed or unused builds

This cleanup is usually automated using:

* Nexus lifecycle policies
* Scheduled cleanup scripts
* Jenkins cleanup jobs

---

## 1Ô∏è‚É£4Ô∏è‚É£ Handling Large Number of Images in Nexus

**Answer:**

At month-end, when many images accumulate:

* We rely on **image tagging**

  * `latest`
  * semantic versions
  * environment-specific tags (prod-release)
* Production images are **never auto-deleted**
* DEV and UAT images follow aggressive cleanup

Retention differs by environment:

* DEV ‚Üí short retention
* UAT ‚Üí medium retention
* PROD ‚Üí long-term retention

This balances **storage cost and traceability**.

---

## 1Ô∏è‚É£5Ô∏è‚É£ Dockerfile for Node.js ‚Äì Best Practices

**Answer:**

We use **multi-stage Docker builds** for Node.js.

Typical choices:

* Base image: `node:18-alpine`
* Reason:

  * Smaller size
  * Faster build
  * Security-friendly

Steps:

* First stage:

  * Install dependencies
  * Build application
* Second stage:

  * Copy only build artifacts
  * Exclude dev dependencies

Final image size is usually **under 150‚Äì200 MB**, sometimes even smaller.

---

## 1Ô∏è‚É£6Ô∏è‚É£ Managing `node_modules`

**Answer:**

We **do not copy `node_modules` from local machines**.

Best practice:

* Use `package-lock.json`
* Run `npm ci` during build
* Cache dependencies at Docker layer level

This ensures:

* Clean, reproducible builds
* No local dependency mismatch
* Faster rebuilds due to Docker layer caching

---

## 1Ô∏è‚É£7Ô∏è‚É£ Handling Large Docker Image Size

**Answer:**

If the image becomes large, we:

* Switch to `alpine` base images
* Use multi-stage builds
* Remove unnecessary tools and files
* Exclude dev dependencies
* Use `.dockerignore`

We also scan images using:

* Trivy
* Snyk

This improves **performance, security, and cost efficiency**.

---

## 1Ô∏è‚É£8Ô∏è‚É£ Terraform ‚Äì What Do You Use It For?

**Answer:**

We use Terraform for:

* Provisioning cloud infrastructure
* Managing Kubernetes clusters
* Creating networking components
* IAM roles and policies
* Load balancers, databases, EC2 instances

Terraform allows us to manage infrastructure as **code**, ensuring:

* Consistency
* Version control
* Easy recreation

---

## 1Ô∏è‚É£9Ô∏è‚É£ Real Terraform Use Case

**Answer:**

Recently, I used Terraform to:

* Create EKS cluster
* Configure node groups
* Setup VPC, subnets, route tables
* Create IAM roles
* Integrate load balancers

We used:

* Remote backend (S3 + DynamoDB)
* Variables for environment-specific values
* Modules for reusability

---

## 2Ô∏è‚É£0Ô∏è‚É£ Terraform Workspaces

**Answer:**

Yes, I have used Terraform workspaces.

Workspaces help manage:

* DEV
* UAT
* PROD

Using the same codebase with:

* Different state files
* Different variable values

This avoids code duplication and keeps infrastructure consistent across environments.

---

## 2Ô∏è‚É£1Ô∏è‚É£ Differentiating Environments in Terraform

**Answer:**

We differentiate environments in Terraform using a combination of:

* **Variables**
* **Workspaces**
* **Separate state files**
* **Environment-specific tfvars**

Typical setup:

* `variables.tf` ‚Üí common variables
* `dev.tfvars`, `uat.tfvars`, `prod.tfvars` ‚Üí environment values
* Remote backend with separate state per environment

Example differences:

* Instance size
* Auto-scaling limits
* Number of replicas
* Network CIDR ranges

This ensures the **same Terraform code** works across environments while keeping configurations isolated.

---

## 2Ô∏è‚É£2Ô∏è‚É£ Multi-Environment Management Using Terraform

**Answer:**

Multi-environment management is done using:

* A **single Terraform codebase**
* Environment-specific inputs
* Separate backends or workspaces

Flow:

* Same modules reused
* Different variable values per environment
* Separate state to avoid conflicts

This avoids:

* Code duplication
* Configuration drift
* Manual infrastructure changes

Everything is controlled through **Git + Terraform plan/apply**.

---

## 2Ô∏è‚É£3Ô∏è‚É£ Writing Reusable Terraform Modules (Example: EC2)

**Answer:**

Yes, I am responsible for writing Terraform modules.

Best practices I follow:

* One resource type per module
* Clear input/output variables
* No hardcoded values
* Use defaults where possible
* Proper tagging standards
* Versioned modules

EC2 module structure:

* `main.tf` ‚Üí EC2 resource
* `variables.tf` ‚Üí configurable inputs
* `outputs.tf` ‚Üí instance ID, IP, etc.

This allows the same module to be reused across:

* DEV
* UAT
* PROD
* Multiple projects

---

## 2Ô∏è‚É£4Ô∏è‚É£ Terraform Module Standardization

**Answer:**

We standardize modules by:

* Enforcing naming conventions
* Mandatory tagging (env, owner, cost-center)
* Version control for modules
* Documentation for usage
* Code reviews before changes

This helps:

* Reduce errors
* Improve reuse
* Maintain consistency across teams

---

## 2Ô∏è‚É£5Ô∏è‚É£ Python ‚Äì Exception Handling

**Answer:**

In Python, exceptions are used to handle **unexpected runtime errors** gracefully.

Common examples:

* File not found
* Network failure
* Invalid input

We use:

* `try`, `except`, `finally`
* Custom exceptions when needed

Exceptions should be used for **exceptional cases**, not normal logic flow.

---

## 2Ô∏è‚É£6Ô∏è‚É£ Python Exception ‚Äì Simple Example

**Answer:**

Example:

```python
try:
    file = open("data.txt", "r")
    content = file.read()
except FileNotFoundError:
    print("File not found")
finally:
    print("Execution completed")
```

Where to use exceptions:

* External dependencies
* I/O operations
* API calls

Where NOT to use exceptions:

* Normal conditional checks
* Business logic decisions

---

## 2Ô∏è‚É£7Ô∏è‚É£ Reading a File Line by Line in Python

**Answer:**

The best way is using a context manager:

```python
with open("abc.txt", "r") as file:
    for line in file:
        print(line.strip())
```

Why:

* Memory efficient
* Automatically closes file
* Clean and readable code

---

## 2Ô∏è‚É£8Ô∏è‚É£ Experience with AWS Lambda

**Answer:**

Yes, I have worked with **AWS Lambda functions**.

Use cases:

* Event-driven automation
* File processing in S3
* Scheduled jobs
* API backends
* CI/CD automation tasks

Lambda is serverless, so:

* No server management
* Auto scaling
* Pay only for execution time

---

## 2Ô∏è‚É£9Ô∏è‚É£ AWS Lambda Handlers ‚Äì How They Work

**Answer:**

A Lambda handler is the **entry point** of the function.

Format:

```python
def lambda_handler(event, context):
    return "Hello World"
```

* `event` ‚Üí input data (API, S3, SNS, etc.)
* `context` ‚Üí runtime metadata

When an event occurs:

* AWS invokes the handler
* Code executes
* Output is returned or passed to the next service

---




